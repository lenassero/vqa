#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Class to evaluate a Visual Question Answering model (adapted from the 
VQA API).
"""
import matplotlib.pyplot as plt
import skimage.io as io
import pandas as pd
import random
import os

from vqa_api.PythonEvaluationTools.vqaEvaluation.vqaEval import VQAEval
from tools import img_file, img_dir

class Evaluate():

    def __init__(self, vqa, results_file, quesFile_test, dataDir, dataType, 
                 dataSubType):
        """Summary
        
        Parameters
        ----------
        vqa : object
            VQA object for the dataSubType to evaluate.
        results_file : str
            Results file path generated by the model to evaluate (on the 
            dataSubType).
        quesFile_test : str
            Questions file path for the dataSubType to evaluate.
        dataDir : str
            Data directory path.
        dataType : str
            Data type.
            "mscoco" only for v1.0. "mscoco" for real and "abstract_v002" 
            for abstract for v1.0. 
        dataSubType : str
            DataSubType to evaluate.
        """
        self.vqa = vqa
        self.results_file = results_file
        self.quesFile_test = quesFile_test
        self.dataDir = dataDir
        self.dataType = dataType
        self.dataSubType = dataSubType

    def compute_accuracies(self):
        """Compute the accuracies for the model: overall accuracy (displayed), 
        accuracy per answer type, accuracy per question type (stored in 
        dataframes).
        """
        # Create vqaRes object
        self.vqaRes = self.vqa.loadRes(self.results_file, self.quesFile_test)   

        # Create vqaEval object by taking vqa and vqaRes
        self.vqaEval = VQAEval(self.vqa, self.vqaRes, n=2)   

        # Evaluate results
        self.vqaEval.evaluate() 

        print("\n")
        print("Overall Accuracy: {}%".format(self.vqaEval.accuracy["overall"]))

        # Accuracies per answer types
        self.acc_answer_type_df = pd.DataFrame.from_dict(
            self.vqaEval.accuracy["perAnswerType"], orient="index").reset_index()
        self.acc_answer_type_df.columns = ["Answer type", "Accuracy"]
        self.acc_answer_type_df = self.acc_answer_type_df.sort_values(
            by="Accuracy", ascending=False).reset_index(drop=True)

        # Accuracies per question types
        self.acc_question_type_df = pd.DataFrame.from_dict(
            self.vqaEval.accuracy["perQuestionType"], orient="index").reset_index()
        self.acc_question_type_df.columns = ["Question type", "Accuracy"]
        self.acc_question_type_df = self.acc_question_type_df.sort_values(
            by="Accuracy", ascending=False).reset_index(drop=True)
        self.acc_question_type_df

    def retrieve_result(self, sup_score=100, answer_type=None, question_type=None):
        """Retrieve a random result of the dataSubType that has been evaluated. 
        It selects randomly an image/question pair (corresponding to a particular 
        answer type or question type) with an accuracy lower than the specified 
        one and displays the ground truth answers, the predicted answer by the 
        model and the image. 
        
        Parameters
        ----------
        sup_score : int, optional
            Upper bound for the accuracy of the image/question pair to retrieve.
        answer_type : None, optional
            Answer type (str) of the answer to the image/question pair to 
            retrieve.
        question_type : None, optional
            Question type of the image/question pair to retrieve.
        
        Raises
        ------
        ValueError
            Only answer_type or question_type can be specified, not both.
        """
        if not answer_type and not question_type:
            evals = [quesId for quesId in self.vqaEval.evalQA 
                     if self.vqaEval.evalQA[quesId]<=sup_score]   

        if answer_type and question_type:
            raise ValueError("answer_type and question_type cannot be both"
                             " specified. Choose either one.")
        if answer_type:
            evals = [quesId for quesId in self.vqaEval.evalAnsType[answer_type] 
                     if self.vqaEval.evalQA[quesId]<=sup_score]  

        if question_type:
            evals = [quesId for quesId in self.vqaEval.evalQuesType[question_type] 
                     if self.vqaEval.evalQA[quesId]<=sup_score]  

        if len(evals) > 0:
            print("Ground truth answers:")
            print("---------------------")
            randomEval = random.choice(evals)
            randomAnn = self.vqa.loadQA(randomEval)
            self.vqa.showQA(randomAnn)

            print("\n")
            print("Generated answer (accuracy {}%):".format(self.vqaEval.evalQA[randomEval]))
            print("---------------------------------")
            ann = self.vqaRes.loadQA(randomEval)[0]
            print("Answer: {}".format(ann["answer"]))

            imgId = randomAnn[0]["image_id"]
            imgDir = img_dir(self.dataDir, self.dataType, self.dataSubType)
            imgFilename = img_file(self.dataSubType, imgId)
            imgPath = os.path.join(imgDir, imgFilename)
            if os.path.isfile(imgPath):
                I = io.imread(imgPath)
                plt.imshow(I)
                plt.axis("off")
                plt.show()
