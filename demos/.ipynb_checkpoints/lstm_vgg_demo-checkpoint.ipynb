{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#VGG16-model\" data-toc-modified-id=\"VGG16-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>VGG16 model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Embedding-of-COCO-images\" data-toc-modified-id=\"Embedding-of-COCO-images-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Embedding of COCO images</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#LSTM-+-VGG\" data-toc-modified-id=\"LSTM-+-VGG-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM + VGG</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Index-answers\" data-toc-modified-id=\"Index-answers-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Index answers</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-text-data\" data-toc-modified-id=\"Prepare-text-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Prepare text data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data\" data-toc-modified-id=\"Prepare-image-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Prepare image data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-definition\" data-toc-modified-id=\"Model-definition-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Model definition</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Training\" data-toc-modified-id=\"Training-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path of the main directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from tools import img_dir, img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model layers \n",
    "layers = vgg_model.layers\n",
    "for i in range(len(layers)):\n",
    "    print \"--> Layer {}\".format(i+1), \"\\n\"\n",
    "    print layers[i].get_config(), \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fc7 layer is the layer 22, which name is fc2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = layers[21]\n",
    "print fc7_layer.get_config()\n",
    "fc7_layer_name = fc7_layer.get_config()[\"name\"]\n",
    "print fc7_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape and output shape of the fc7 layer\n",
    "print \"Input shape:\", fc7_layer.input_shape\n",
    "print \"Ouput shape:\", fc7_layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the fc7 layer\n",
    "# Weights shape\n",
    "print \"Weights shape:\", fc7_layer.get_weights()[0].shape\n",
    "# Bias shape\n",
    "print \"Bias shape:\", fc7_layer.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Layer by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = vgg_model.get_layer(fc7_layer_name)\n",
    "fc7_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of COCO images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "dataType = \"mscoco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids = [(9, \"train2014\"), (25, \"train2014\")]\n",
    "# List to store the arrays for images\n",
    "imgs = []\n",
    "for image_id in image_ids:\n",
    "    # Resize the images as VGG inputs\n",
    "    img = image.load_img(os.path.join(img_dir(dataDir, dataType, \n",
    "        image_id[1]), img_file(image_id[1], image_id[0])), target_size=(224, 224)) \n",
    "    img = image.img_to_array(img)\n",
    "    imgs.append(img)\n",
    "imgs = np.stack(imgs)\n",
    "# Preprocess the images corresponding to VGG\n",
    "imgs = preprocess_input(imgs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images embedding using the fc7 layer of VGG16\n",
    "vgg_model_fc7.predict(imgs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_vgg import LSTMVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\\Annotations\\mscoco_train2014_annotations.json\n",
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:13.026000\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:05.011000\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "text_vision = LSTMVGG(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.get_most_common_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.get_top_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.encode_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248349L, 1000L)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_answers_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121512L, 1000L)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.test_answers_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize questions using the vocabulary of the training questions\n",
    "text_vision.tokenize_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248349, 121512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of questions in the train and test set\n",
    "len(text_vision.train_sequences), len(text_vision.test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  7%|▋         | 7/100 [00:00<00:01, 62.50it/s]\n",
      " 14%|█▍        | 14/100 [00:00<00:01, 62.78it/s]\n",
      " 20%|██        | 20/100 [00:00<00:01, 61.54it/s]\n",
      " 26%|██▌       | 26/100 [00:00<00:01, 60.19it/s]\n",
      " 32%|███▏      | 32/100 [00:00<00:01, 59.48it/s]\n",
      " 39%|███▉      | 39/100 [00:00<00:01, 60.09it/s]\n",
      " 46%|████▌     | 46/100 [00:00<00:00, 61.25it/s]\n",
      " 53%|█████▎    | 53/100 [00:00<00:00, 62.13it/s]\n",
      " 61%|██████    | 61/100 [00:00<00:00, 63.54it/s]\n",
      " 69%|██████▉   | 69/100 [00:01<00:00, 64.61it/s]\n",
      " 77%|███████▋  | 77/100 [00:01<00:00, 64.92it/s]\n",
      " 84%|████████▍ | 84/100 [00:01<00:00, 63.40it/s]\n",
      " 91%|█████████ | 91/100 [00:01<00:00, 62.37it/s]\n",
      " 97%|█████████▋| 97/100 [00:01<00:00, 61.94it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.31it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 42.74it/s]\n",
      " 10%|█         | 10/100 [00:00<00:02, 44.64it/s]\n",
      " 17%|█▋        | 17/100 [00:00<00:01, 48.99it/s]\n",
      " 23%|██▎       | 23/100 [00:00<00:01, 49.89it/s]\n",
      " 28%|██▊       | 28/100 [00:00<00:01, 48.95it/s]\n",
      " 34%|███▍      | 34/100 [00:00<00:01, 48.92it/s]\n",
      " 40%|████      | 40/100 [00:00<00:01, 50.19it/s]\n",
      " 46%|████▌     | 46/100 [00:00<00:01, 50.49it/s]\n",
      " 53%|█████▎    | 53/100 [00:01<00:00, 51.76it/s]\n",
      " 59%|█████▉    | 59/100 [00:01<00:00, 52.21it/s]\n",
      " 65%|██████▌   | 65/100 [00:01<00:00, 51.14it/s]\n",
      " 71%|███████   | 71/100 [00:01<00:00, 50.82it/s]\n",
      " 76%|███████▌  | 76/100 [00:01<00:00, 50.26it/s]\n",
      " 82%|████████▏ | 82/100 [00:01<00:00, 50.68it/s]\n",
      " 89%|████████▉ | 89/100 [00:01<00:00, 51.27it/s]\n",
      " 96%|█████████▌| 96/100 [00:01<00:00, 51.98it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 52.16it/s]"
     ]
    }
   ],
   "source": [
    "text_vision.process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 224L, 224L, 3L)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 224L, 224L, 3L)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximum length for the input questions\n",
    "input_length = 22\n",
    "\n",
    "# Vocabulary size from the training set\n",
    "V = text_vision.vocabulary_size_train\n",
    "input_dim = V + 1\n",
    "\n",
    "# Dimension of the vectors (from the paper)\n",
    "embedding_dim = 300\n",
    "\n",
    "# The output from the embedding layer is equal to input_length x embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 22, 300)      4099800     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         134260544   input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 1024)         5427200     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         4195328     model_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1024)         0           lstm_2[0][0]                     \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1000)         1025000     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1000)         1001000     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1000)         1001000     dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 151,009,872\n",
      "Trainable params: 16,749,328\n",
      "Non-trainable params: 134,260,544\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let\"s get a tensor with the output of our vision model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "# Freeze the vision model weights\n",
    "vgg_model_fc7.trainable = False\n",
    "fc7 = vgg_model_fc7(image_input)\n",
    "\n",
    "# Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# match the questions embedding\n",
    "encoded_image = Dense(1024, activation=\"tanh\")(fc7)\n",
    "\n",
    "# Langage model\n",
    "question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim, \n",
    "                              input_length=input_length)(question_input)\n",
    "encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# Point-wise multiplication of the outputs from the vision model and the \n",
    "# langage model\n",
    "merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# Hidden layers with 0.5 dropouts\n",
    "merge = Dropout(0.5)(merge)\n",
    "hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "hidden1 = Dropout(0.5)(hidden1)\n",
    "hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# Final softmax layer\n",
    "output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# This is our final model:\n",
    "vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Summarize layers\n",
    "print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = [text_vision.train_images, text_vision.train_sequences[:300]]\n",
    "x_val = [text_vision.test_images, text_vision.test_sequences[:300]]\n",
    "y_train = text_vision.train_answers_categorical[:300]\n",
    "y_val = text_vision.test_answers_categorical[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 22L)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "300/300 [==============================] - ETA: 1:41 - loss: 6.9096 - acc: 0.007 - ETA: 24s - loss: 6.6198 - acc: 0.156 - 315s 1s/step - loss: 7.1369 - acc: 0.1700 - val_loss: 11.2683 - val_acc: 0.1367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd1f7f60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = vqa_model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
