{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#VGG16-model\" data-toc-modified-id=\"VGG16-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>VGG16 model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Embedding-of-COCO-images\" data-toc-modified-id=\"Embedding-of-COCO-images-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Embedding of COCO images</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#LSTM-+-VGG\" data-toc-modified-id=\"LSTM-+-VGG-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM + VGG</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Index-answers\" data-toc-modified-id=\"Index-answers-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Index answers</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-text-data\" data-toc-modified-id=\"Prepare-text-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Prepare text data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-the-embedding-layer\" data-toc-modified-id=\"Prepare-the-embedding-layer-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Prepare the embedding layer</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data-I\" data-toc-modified-id=\"Prepare-image-data-I-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Prepare image data I</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data-II-(pass-through-VGG)\" data-toc-modified-id=\"Prepare-image-data-II-(pass-through-VGG)-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Prepare image data II (pass through VGG)</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Sanity-check\" data-toc-modified-id=\"Sanity-check-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Sanity check</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-definition\" data-toc-modified-id=\"Model-definition-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Model definition</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-I-(224x224x3-input)\" data-toc-modified-id=\"Model-I-(224x224x3-input)-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>Model I (224x224x3 input)</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-II-(4096-input-from-VGG)\" data-toc-modified-id=\"Model-II-(4096-input-from-VGG)-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Model II (4096 input from VGG)</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Training\" data-toc-modified-id=\"Training-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path of the main directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from tools import img_dir, img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model layers \n",
    "layers = vgg_model.layers\n",
    "for i in range(len(layers)):\n",
    "    print \"--> Layer {}\".format(i+1), \"\\n\"\n",
    "    print layers[i].get_config(), \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fc7 layer is the layer 22, which name is fc2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = layers[21]\n",
    "print fc7_layer.get_config()\n",
    "fc7_layer_name = fc7_layer.get_config()[\"name\"]\n",
    "print fc7_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape and output shape of the fc7 layer\n",
    "print \"Input shape:\", fc7_layer.input_shape\n",
    "print \"Ouput shape:\", fc7_layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the fc7 layer\n",
    "# Weights shape\n",
    "print \"Weights shape:\", fc7_layer.get_weights()[0].shape\n",
    "# Bias shape\n",
    "print \"Bias shape:\", fc7_layer.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Layer by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = vgg_model.get_layer(fc7_layer_name)\n",
    "fc7_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of COCO images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "dataType = \"mscoco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids = [(9, \"train2014\"), (25, \"train2014\")]\n",
    "# List to store the arrays for images\n",
    "imgs = []\n",
    "for image_id in image_ids:\n",
    "    # Resize the images as VGG inputs\n",
    "    img = image.load_img(os.path.join(img_dir(dataDir, dataType, \n",
    "        image_id[1]), img_file(image_id[1], image_id[0])), target_size=(224, 224)) \n",
    "    img = image.img_to_array(img)\n",
    "    imgs.append(img)\n",
    "imgs = np.stack(imgs)\n",
    "# Preprocess the images corresponding to VGG\n",
    "imgs = preprocess_input(imgs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images embedding using the fc7 layer of VGG16\n",
    "vgg_model_fc7.predict(img).flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm_vgg import LSTMVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\\Annotations\\mscoco_train2014_annotations.json\n",
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:14.748000\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.991000\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "text_vision = LSTMVGG(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the top 1000 answers from the training set ...\n",
      "Mapping each top answer to an index between 0 and 999 in self.idx_to_answer_dic and self.answer_to_idx_dic ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.get_top_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the ground truth answer for each question/image in the training and test sets ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.get_most_common_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only every question/image with an answer among the top 1000 answers of the training set ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.reduce_train_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train and test answers ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.encode_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477L, 1000L)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_answers_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the tokenizer on the training questions ...\n",
      "Embedding the train and test questions ...\n",
      "Padding the sequences to a maximum length of 22 ...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions using the vocabulary of the training questions\n",
    "text_vision.tokenize_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477, 121512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of questions in the train and test set\n",
    "len(text_vision.train_sequences), len(text_vision.test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12494L, 300L)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.process_images_train(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data II (pass through VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:11<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%time text_vision.process_encode_images_train(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 4096L)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the different data correspond (question, answer and image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.train_questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.train_answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(text_vision.train_images[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length for the input questions\n",
    "input_length = 22\n",
    "\n",
    "# Vocabulary size from the training set\n",
    "V = text_vision.vocabulary_size_train\n",
    "input_dim = V + 1\n",
    "\n",
    "# Dimension of the vectors (from the paper)\n",
    "embedding_dim = 300\n",
    "\n",
    "# The output from the embedding layer is equal to input_length x embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I (224x224x3 input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let\"s get a tensor with the output of our vision model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "# Freeze the vision model weights\n",
    "vgg_model_fc7.trainable = False\n",
    "fc7 = vgg_model_fc7(image_input)\n",
    "\n",
    "# Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# match the questions embedding\n",
    "encoded_image = Dense(1024, activation=\"tanh\")(fc7)\n",
    "\n",
    "# Langage model\n",
    "question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim,\n",
    "                              weights=[text_vision.embedding_matrix],\n",
    "                              input_length=input_length,\n",
    "                              trainable=False\n",
    "                             )(question_input)\n",
    "encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# Point-wise multiplication of the outputs from the vision model and the \n",
    "# langage model\n",
    "merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# Hidden layers with 0.5 dropouts\n",
    "merge = Dropout(0.5)(merge)\n",
    "hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "hidden1 = Dropout(0.5)(hidden1)\n",
    "hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# Final softmax layer\n",
    "output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# This is our final model:\n",
    "vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Summarize layers\n",
    "print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model II (4096 input from VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser Benab\\Anaconda2\\lib\\site-packages\\keras\\layers\\recurrent.py:1923: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 22, 300)      3748200     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1024)         5427200     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1024)         0           lstm_1[0][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000)         1001000     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1000)         1001000     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,397,728\n",
      "Trainable params: 12,649,528\n",
      "Non-trainable params: 3,748,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let\"s get a tensor with the output of our vision model\n",
    "image_input = Input(shape=(4096,))\n",
    "\n",
    "# Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# match the questions embedding\n",
    "encoded_image = Dense(1024, activation=\"tanh\")(image_input)\n",
    "\n",
    "# Langage model\n",
    "question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim,\n",
    "                              weights=[text_vision.embedding_matrix],\n",
    "                              input_length=input_length,\n",
    "                              trainable=False\n",
    "                             )(question_input)\n",
    "encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# Point-wise multiplication of the outputs from the vision model and the \n",
    "# langage model\n",
    "merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# Hidden layers with 0.5 dropouts\n",
    "merge = Dropout(0.5)(merge)\n",
    "hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "hidden1 = Dropout(0.5)(hidden1)\n",
    "hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# Final softmax layer\n",
    "output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# This is our final model:\n",
    "vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Summarize layers\n",
    "print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables \n",
    "text_vision.clear_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vqa_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split train into a train and val set \n",
    "split_index_1 = 250\n",
    "split_index_2 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = [text_vision.train_images[:split_index_1], text_vision.train_sequences[:split_index_1]]\n",
    "x_val = [text_vision.train_images[split_index_1:split_index_2], text_vision.train_sequences[split_index_1:split_index_2]]\n",
    "y_train = text_vision.train_answers_categorical[:split_index_1]\n",
    "y_val = text_vision.train_answers_categorical[split_index_1:split_index_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250L, 4096L) (250L, 22L) (250L, 1000L)\n",
      "(50L, 4096L) (50L, 22L) (50L, 1000L)\n"
     ]
    }
   ],
   "source": [
    "print x_train[0].shape, x_train[1].shape, y_train.shape\n",
    "print x_val[0].shape, x_val[1].shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 250 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - ETA: 5s - loss: 6.9075 - acc: 0.0000e+0 - 11s 45ms/step - loss: 6.6946 - acc: 0.1760 - val_loss: 10.5812 - val_acc: 0.3200\n"
     ]
    }
   ],
   "source": [
    "history = vqa_model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = vqa_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = text_vision.predictions_to_dic(predictions, text_vision.train_questions_ids[split_index_1:split_index_2])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_ind_val_true = np.argmax(y_val, axis=1)\n",
    "idx_to_answer = np.vectorize(text_vision.idx_to_answer)\n",
    "idx_to_answer(answers_ind_val_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.66666599999999,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
