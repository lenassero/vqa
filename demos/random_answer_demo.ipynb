{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path of the main directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "from vqa_api.PythonEvaluationTools.vqaEvaluation.vqaEval import VQAEval\n",
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA\n",
    "from random_answer import RandomAnswer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir     = r'C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa'\n",
    "versionType = '' \n",
    "taskType    = 'OpenEnded' \n",
    "dataType    = 'mscoco'\n",
    "dataSubTypes = ['train2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:11.690000\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# First baseline method: random answer\n",
    "RA = RandomAnswer(dataDir, versionType, taskType, dataType, dataSubTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the top answers \n",
    "RA.get_top_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'yes', u'no', u'2', u'1', u'white']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 answers\n",
    "RA.top_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:04.978000\n",
      "creating index...\n",
      "index created!\n",
      "--> Saving the results\n"
     ]
    }
   ],
   "source": [
    "# Predict answers on the validation dataset\n",
    "dataSubType = \"val2014\"\n",
    "RA.predict(dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomAnswer results file: C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa/Results/OpenEnded_mscoco_val2014_RandomAnswer_results.json\n"
     ]
    }
   ],
   "source": [
    "# Name of the baseline method used\n",
    "method_name = RA.__class__.__name__\n",
    "\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType']\n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = \\\n",
    "['%s/Results/%s%s_%s_%s_%s_%s.json'%(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "method_name, fileType) for fileType in fileTypes] \n",
    "print \"{} results file:\".format(method_name), resFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=0.67s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# vqa object for the test \n",
    "vqa = RA.vqa_test\n",
    "# Create vqaRes object\n",
    "vqaRes = vqa.loadRes(resFile, RA.quesFile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% "
     ]
    }
   ],
   "source": [
    "# Evaluate results\n",
    "vqaEval.evaluate() \n",
    "\n",
    "# Print accuracies\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
    "print(\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "    print(\"%s : %.02f\" %(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print(\"\\n\")\n",
    "print(\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "    print(\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "# 35 is per question percentage accuracy\n",
    "# evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   \n",
    "# if len(evals) > 0:\n",
    "#     print('ground truth answers')\n",
    "#     randomEval = random.choice(evals)\n",
    "#     randomAnn = vqa.loadQA(randomEval)\n",
    "#     vqa.showQA(randomAnn)\n",
    "\n",
    "#     print('\\n')\n",
    "#     print('generated answer (accuracy %.02f)'%(vqaEval.evalQA[randomEval]))\n",
    "#     ann = vqaRes.loadQA(randomEval)[0]\n",
    "#     print(\"Answer:   %s\\n\" %(ann['answer']))\n",
    "\n",
    "#     imgId = randomAnn[0]['image_id']\n",
    "#     imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "#     if os.path.isfile(imgDir + imgFilename):\n",
    "#         I = io.imread(imgDir + imgFilename)\n",
    "#         plt.imshow(I)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# # plot accuracy for various question types\n",
    "# plt.bar(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#         list(vqaEval.accuracy['perQuestionType'].values()), align='center')\n",
    "# plt.xticks(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#            list(vqaEval.accuracy['perQuestionType'].keys()), \n",
    "#            rotation='0',fontsize=10)\n",
    "# plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "# plt.xlabel('Question Types', fontsize=10)\n",
    "# plt.ylabel('Accuracy', fontsize=10)\n",
    "# plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "# json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "# json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "# json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "# json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
