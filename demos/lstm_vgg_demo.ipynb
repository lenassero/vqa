{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#VGG16-model\" data-toc-modified-id=\"VGG16-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>VGG16 model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Embedding-of-COCO-images\" data-toc-modified-id=\"Embedding-of-COCO-images-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Embedding of COCO images</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#LSTM-+-VGG\" data-toc-modified-id=\"LSTM-+-VGG-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM + VGG</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Index-answers\" data-toc-modified-id=\"Index-answers-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Index answers</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-text-data\" data-toc-modified-id=\"Prepare-text-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Prepare text data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-the-embedding-layer\" data-toc-modified-id=\"Prepare-the-embedding-layer-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Prepare the embedding layer</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data-I\" data-toc-modified-id=\"Prepare-image-data-I-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Prepare image data I</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data-II-(pass-through-VGG)\" data-toc-modified-id=\"Prepare-image-data-II-(pass-through-VGG)-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Prepare image data II (pass through VGG)</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Sanity-check\" data-toc-modified-id=\"Sanity-check-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Sanity check</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-definition\" data-toc-modified-id=\"Model-definition-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Model definition</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-I-(224x224x3-input)\" data-toc-modified-id=\"Model-I-(224x224x3-input)-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>Model I (224x224x3 input)</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-II-(4096-input-from-VGG)\" data-toc-modified-id=\"Model-II-(4096-input-from-VGG)-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Model II (4096 input from VGG)</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Training\" data-toc-modified-id=\"Training-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Predicting-on-test-data-(val2014)\" data-toc-modified-id=\"Predicting-on-test-data-(val2014)-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Predicting on test data (val2014)</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Preparing-test-data\" data-toc-modified-id=\"Preparing-test-data-2.9.1\"><span class=\"toc-item-num\">2.9.1&nbsp;&nbsp;</span>Preparing test data</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path of the main directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, multiply, concatenate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from tools import img_dir, img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model layers \n",
    "layers = vgg_model.layers\n",
    "for i in range(len(layers)):\n",
    "    print \"--> Layer {}\".format(i+1), \"\\n\"\n",
    "    print layers[i].get_config(), \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fc7 layer is the layer 22, which name is fc2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = layers[21]\n",
    "print fc7_layer.get_config()\n",
    "fc7_layer_name = fc7_layer.get_config()[\"name\"]\n",
    "print fc7_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape and output shape of the fc7 layer\n",
    "print \"Input shape:\", fc7_layer.input_shape\n",
    "print \"Ouput shape:\", fc7_layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights of the fc7 layer\n",
    "# Weights shape\n",
    "print \"Weights shape:\", fc7_layer.get_weights()[0].shape\n",
    "# Bias shape\n",
    "print \"Bias shape:\", fc7_layer.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Layer by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = vgg_model.get_layer(fc7_layer_name)\n",
    "fc7_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of COCO images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "dataType = \"mscoco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [(9, \"train2014\"), (25, \"train2014\")]\n",
    "# List to store the arrays for images\n",
    "imgs = []\n",
    "for image_id in image_ids:\n",
    "    # Resize the images as VGG inputs\n",
    "    img = image.load_img(os.path.join(img_dir(dataDir, dataType, \n",
    "        image_id[1]), img_file(image_id[1], image_id[0])), target_size=(224, 224)) \n",
    "    img = image.img_to_array(img)\n",
    "    imgs.append(img)\n",
    "imgs = np.stack(imgs)\n",
    "# Preprocess the images corresponding to VGG\n",
    "imgs = preprocess_input(imgs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(1, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images embedding using the fc7 layer of VGG16\n",
    "vgg_model_fc7.predict(img).flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_vgg import LSTMVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "dataDir = \"/home/ubuntu/data/vqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:11.495946\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "\n",
      "---> val2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:04.737271\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "text_vision = LSTMVGG(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the top 1000 answers from the training set ...\n",
      "Mapping each top answer to an index between 0 and 999 in self.idx_to_answer_dic and self.answer_to_idx_dic ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.get_top_answers_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the ground truth answer for each question/image ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.get_most_common_answer_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping only every question/image with an answer among the top 1000 answers of the training set ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.reduce_answers_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding train answers ...\n"
     ]
    }
   ],
   "source": [
    "text_vision.encode_answers_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_answers_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the tokenizer on the training questions ...\n",
      "Embedding the train questions ...\n",
      "Padding the sequences to a maximum length of 22 ...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions using the vocabulary of the training questions\n",
    "text_vision.tokenize_questions_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214477"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of questions in the train set\n",
    "len(text_vision.train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12494, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_vision.process_images_train(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data II (pass through VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have already been encoded, opening them ...\n",
      "CPU times: user 1.34 s, sys: 836 ms, total: 2.17 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%time text_vision.process_encode_images_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the different data correspond (question, answer and image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.train_questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.train_answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(text_vision.train_images[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length for the input questions\n",
    "input_length = 22\n",
    "\n",
    "# Vocabulary size from the training set\n",
    "V = text_vision.vocabulary_size_train\n",
    "input_dim = V + 1\n",
    "\n",
    "# Dimension of the vectors (from the paper)\n",
    "embedding_dim = 300\n",
    "\n",
    "# The output from the embedding layer is equal to input_length x embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I (224x224x3 input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VGG16 model\n",
    "# vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "#                                input_tensor=None, input_shape=None, \n",
    "#                                pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"Submodel\" of VGG until the fc7 layer\n",
    "# vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let\"s get a tensor with the output of our vision model\n",
    "# image_input = Input(shape=(224, 224, 3))\n",
    "# # Freeze the vision model weights\n",
    "# vgg_model_fc7.trainable = False\n",
    "# fc7 = vgg_model_fc7(image_input)\n",
    "\n",
    "# # Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# # match the questions embedding\n",
    "# encoded_image = Dense(1024, activation=\"tanh\")(fc7)\n",
    "\n",
    "# # Langage model\n",
    "# question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "# embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim,\n",
    "#                               weights=[text_vision.embedding_matrix],\n",
    "#                               input_length=input_length,\n",
    "#                               trainable=False\n",
    "#                              )(question_input)\n",
    "# encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# # Point-wise multiplication of the outputs from the vision model and the \n",
    "# # langage model\n",
    "# merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# # Hidden layers with 0.5 dropouts\n",
    "# merge = Dropout(0.5)(merge)\n",
    "# hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "# hidden1 = Dropout(0.5)(hidden1)\n",
    "# hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# # Final softmax layer\n",
    "# output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# # This is our final model:\n",
    "# vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# # Summarize layers\n",
    "# print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model II (4096 input from VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 22, 300)      3748200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1024)         5427200     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1024)         0           lstm_1[0][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000)         1001000     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1000)         1001000     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,397,728\n",
      "Trainable params: 16,397,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let\"s get a tensor with the output of our vision model\n",
    "image_input = Input(shape=(4096,))\n",
    "\n",
    "# Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# match the questions embedding\n",
    "# encoded_image = image_input\n",
    "encoded_image = Dense(1024, activation=\"tanh\")(image_input)\n",
    "\n",
    "# Langage model\n",
    "question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim,\n",
    "                              weights=[text_vision.embedding_matrix],\n",
    "                              input_length=input_length)(question_input)\n",
    "encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# Point-wise multiplication of the outputs from the vision model and the \n",
    "# langage model\n",
    "# merge = concatenate([encoded_question, encoded_image])\n",
    "merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# Hidden layers with 0.5 dropouts\n",
    "merge = Dropout(0.5)(merge)\n",
    "hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "hidden1 = Dropout(0.5)(hidden1)\n",
    "hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# Final softmax layer\n",
    "output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# This is our final model:\n",
    "vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Summarize layers\n",
    "print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete variables \n",
    "text_vision.clear_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214477, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Training set to split\n",
    "x = [text_vision.train_images, text_vision.train_sequences]\n",
    "y = text_vision.train_answers_categorical\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set into train set (80%=171580) and validation set\n",
    "x_train = [x[0][:171580], x[1][:171580]]\n",
    "x_val = [x[0][171580:], x[1][171580:]]\n",
    "y_train = y[:171580]\n",
    "y_val = y[171580:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split training set into train set (80%) and validation set (20%)\n",
    "# rs = ShuffleSplit(n_splits=1, test_size=.2, random_state=0)\n",
    "# rs.get_n_splits(x[0])\n",
    "# train_indices = list(rs.split(x[0]))[0][0] \n",
    "# val_indices = list(rs.split(x[0]))[0][1]\n",
    "\n",
    "# x_train = [x[0][train_indices], x[1][train_indices]]\n",
    "# x_val = [x[0][val_indices], x[1][val_indices]]\n",
    "# y_train = y[train_indices]\n",
    "# y_val = y[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "model_name = \"lstm-vgg-0.8train\"\n",
    "model_dir = os.path.join(text_vision.dataDir, \"Model\", \"LSTM_VGG\")\n",
    "filepath = os.path.join(model_dir, \"_\".join((model_name, \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"))) \n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=0, save_best_only=True, mode=\"max\", period=10)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "history = vqa_model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                        epochs=100, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(os.path.join(model_dir, \"model_accuracy_train_val.jpg\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.savefig(os.path.join(model_dir, \"model_loss_train_val.jpg\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "model_suffix = \"0.8train_30_epochs\"\n",
    "vqa_model.save(os.path.join(text_vision.dataDir, \"Model\", \"lstm_vgg_{}.h5\".format(model_suffix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "model_name = \"lstm-vgg-0.8train\"\n",
    "model_dir = os.path.join(text_vision.dataDir, \"Model\", \"LSTM_VGG\")\n",
    "model_path = os.path.join(model_dir, \"_\".join((model_name, \"weights-improvement-90-0.51.hdf5\"))) \n",
    "vqa_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the whole train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "model_name = \"lstm-vgg-train\"\n",
    "model_dir = os.path.join(text_vision.dataDir, \"Model\", \"LSTM_VGG\")\n",
    "filepath = os.path.join(model_dir, \"_\".join((model_name, \"weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"))) \n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"acc\", verbose=0, save_best_only=True, mode=\"max\", period=10)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 2.9957 - acc: 0.3279\n",
      "Epoch 2/50\n",
      "214477/214477 [==============================] - 226s 1ms/step - loss: 2.2181 - acc: 0.4083\n",
      "Epoch 3/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 2.0630 - acc: 0.4533\n",
      "Epoch 4/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.9916 - acc: 0.4753\n",
      "Epoch 5/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.9496 - acc: 0.4876\n",
      "Epoch 6/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.9197 - acc: 0.4986\n",
      "Epoch 7/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.8959 - acc: 0.5074\n",
      "Epoch 8/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.8782 - acc: 0.5145\n",
      "Epoch 9/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.8535 - acc: 0.5249\n",
      "Epoch 10/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.8327 - acc: 0.5338\n",
      "Epoch 11/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.8167 - acc: 0.5409\n",
      "Epoch 12/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7958 - acc: 0.5498\n",
      "Epoch 13/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7813 - acc: 0.5555\n",
      "Epoch 14/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7711 - acc: 0.5608\n",
      "Epoch 15/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7555 - acc: 0.5681\n",
      "Epoch 16/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7411 - acc: 0.5728\n",
      "Epoch 17/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7277 - acc: 0.5776\n",
      "Epoch 18/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7211 - acc: 0.5817\n",
      "Epoch 19/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.7143 - acc: 0.5853\n",
      "Epoch 20/50\n",
      "214477/214477 [==============================] - 228s 1ms/step - loss: 1.7004 - acc: 0.5900\n",
      "Epoch 21/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6965 - acc: 0.5911\n",
      "Epoch 22/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6850 - acc: 0.5958\n",
      "Epoch 23/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6795 - acc: 0.5978\n",
      "Epoch 24/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6704 - acc: 0.6009\n",
      "Epoch 25/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6697 - acc: 0.6027\n",
      "Epoch 26/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6625 - acc: 0.6034\n",
      "Epoch 27/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6595 - acc: 0.6055\n",
      "Epoch 28/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6544 - acc: 0.6063\n",
      "Epoch 29/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6504 - acc: 0.6080\n",
      "Epoch 30/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6481 - acc: 0.6085\n",
      "Epoch 31/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6437 - acc: 0.6108\n",
      "Epoch 32/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6376 - acc: 0.6107\n",
      "Epoch 33/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6376 - acc: 0.6112\n",
      "Epoch 34/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6329 - acc: 0.6113\n",
      "Epoch 35/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6327 - acc: 0.6124\n",
      "Epoch 36/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6249 - acc: 0.6158\n",
      "Epoch 37/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6227 - acc: 0.6163\n",
      "Epoch 38/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6264 - acc: 0.6152\n",
      "Epoch 39/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6286 - acc: 0.6146\n",
      "Epoch 40/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6290 - acc: 0.6139\n",
      "Epoch 41/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6231 - acc: 0.6164\n",
      "Epoch 42/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6176 - acc: 0.6177\n",
      "Epoch 43/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6204 - acc: 0.6177\n",
      "Epoch 44/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6224 - acc: 0.6158\n",
      "Epoch 45/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6273 - acc: 0.6154\n",
      "Epoch 46/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6237 - acc: 0.6162\n",
      "Epoch 47/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6288 - acc: 0.6155\n",
      "Epoch 48/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6316 - acc: 0.6137\n",
      "Epoch 49/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6297 - acc: 0.6143\n",
      "Epoch 50/50\n",
      "214477/214477 [==============================] - 227s 1ms/step - loss: 1.6323 - acc: 0.6115\n"
     ]
    }
   ],
   "source": [
    "# Fit the models\n",
    "history = vqa_model.fit(x, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on test data (val2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "model_name = \"lstm-vgg-train\"\n",
    "# model_name = \"lstm_vgg_train\"\n",
    "model_dir = os.path.join(text_vision.dataDir, \"Model\", \"LSTM_VGG\")\n",
    "model_path = os.path.join(model_dir, \"_\".join((model_name, \"weights-improvement-30-0.61.hdf5\"))) \n",
    "# model_path = os.path.join(os.path.join(text_vision.dataDir, \"Model\"), \"_\".join((model_name, \"50_epochs.h5\"))) \n",
    "vqa_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding the test questions ...\n",
      "Padding the sequences to a maximum length of 22 ...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions using the vocabulary of the training questions\n",
    "text_vision.tokenize_questions_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of questions in the train set\n",
    "len(text_vision.test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have already been encoded, opening them ...\n",
      "CPU times: user 896 ms, sys: 524 ms, total: 1.42 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "# Encode images\n",
    "%time text_vision.process_encode_images_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121512, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input to the \"LSTM + CNN\" \n",
    "x_test = [text_vision.test_images, \n",
    "         text_vision.test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vqa_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Saving the results\n"
     ]
    }
   ],
   "source": [
    "res = text_vision.predictions_to_dic(predictions, text_vision.test_questions_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=0.61s)\n",
      "creating index...\n",
      "index created!\n",
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% Done computing accuracy\n",
      "\n",
      "\n",
      "Overall Accuracy: 50.64%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracies for the validation set\n",
    "eval_ = Evaluate(text_vision.vqa_test, text_vision.results_file, text_vision.quesFile_test, text_vision.dataDir, \n",
    "                 text_vision.dataType, text_vision.dataSubTypeTest)\n",
    "eval_.compute_accuracies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.66666599999999,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
