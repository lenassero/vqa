{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#VGG16-model\" data-toc-modified-id=\"VGG16-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>VGG16 model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Embedding-of-COCO-images\" data-toc-modified-id=\"Embedding-of-COCO-images-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Embedding of COCO images</a></span></li></ul></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#LSTM-+-VGG\" data-toc-modified-id=\"LSTM-+-VGG-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSTM + VGG</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Index-answers\" data-toc-modified-id=\"Index-answers-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Index answers</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-text-data\" data-toc-modified-id=\"Prepare-text-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Prepare text data</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Prepare-image-data\" data-toc-modified-id=\"Prepare-image-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Prepare image data</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Sanity-check\" data-toc-modified-id=\"Sanity-check-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Sanity check</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Model-definition\" data-toc-modified-id=\"Model-definition-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Model definition</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/Documents/git/vqa/demos/lstm_vgg_demo.ipynb#Training\" data-toc-modified-id=\"Training-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Training</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path of the main directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from tools import img_dir, img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model layers \n",
    "layers = vgg_model.layers\n",
    "for i in range(len(layers)):\n",
    "    print \"--> Layer {}\".format(i+1), \"\\n\"\n",
    "    print layers[i].get_config(), \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fc7 layer is the layer 22, which name is fc2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = layers[21]\n",
    "print fc7_layer.get_config()\n",
    "fc7_layer_name = fc7_layer.get_config()[\"name\"]\n",
    "print fc7_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input shape and output shape of the fc7 layer\n",
    "print \"Input shape:\", fc7_layer.input_shape\n",
    "print \"Ouput shape:\", fc7_layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights of the fc7 layer\n",
    "# Weights shape\n",
    "print \"Weights shape:\", fc7_layer.get_weights()[0].shape\n",
    "# Bias shape\n",
    "print \"Bias shape:\", fc7_layer.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a Layer by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fc7 layer\n",
    "fc7_layer = vgg_model.get_layer(fc7_layer_name)\n",
    "fc7_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of COCO images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "dataType = \"mscoco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids = [(9, \"train2014\"), (25, \"train2014\")]\n",
    "# List to store the arrays for images\n",
    "imgs = []\n",
    "for image_id in image_ids:\n",
    "    # Resize the images as VGG inputs\n",
    "    img = image.load_img(os.path.join(img_dir(dataDir, dataType, \n",
    "        image_id[1]), img_file(image_id[1], image_id[0])), target_size=(224, 224)) \n",
    "    img = image.img_to_array(img)\n",
    "    imgs.append(img)\n",
    "imgs = np.stack(imgs)\n",
    "# Preprocess the images corresponding to VGG\n",
    "imgs = preprocess_input(imgs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Images embedding using the fc7 layer of VGG16\n",
    "vgg_model_fc7.predict(imgs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm_vgg import LSTMVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\\Annotations\\mscoco_train2014_annotations.json\n",
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:13.580000\n",
      "creating index...\n",
      "index created!\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:07.014000\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "text_vision = LSTMVGG(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.get_top_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.get_most_common_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': u'yes',\n",
       " 'data_subtype': 'train2014',\n",
       " u'image_id': 487025,\n",
       " u'question': u'Is there a shadow?',\n",
       " u'question_id': 4870251}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.questions_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vision.reduce_train_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_vision.encode_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477L, 1000L)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_answers_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the tokenizer on the training questions ...\n",
      "Embedding the train and test questions ...\n",
      "Padding the sequences to a maximum length of 22 ...\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions using the vocabulary of the training questions\n",
    "text_vision.tokenize_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214477, 121512)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of questions in the train and test set\n",
    "len(text_vision.train_sequences), len(text_vision.test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 210.08it/s]\n",
      "100%|██████████| 300/300 [00:01<00:00, 251.26it/s]\n"
     ]
    }
   ],
   "source": [
    "text_vision.process_images(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 224L, 224L, 3L)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 224L, 224L, 3L)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300L, 224L, 224L, 3L)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the different data correspond (question, answer and image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(100))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Is the person on the snowboard a man?'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'yes'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vision.train_answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(text_vision.train_images[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximum length for the input questions\n",
    "input_length = 22\n",
    "\n",
    "# Vocabulary size from the training set\n",
    "V = text_vision.vocabulary_size_train\n",
    "input_dim = V + 1\n",
    "\n",
    "# Dimension of the vectors (from the paper)\n",
    "embedding_dim = 300\n",
    "\n",
    "# The output from the embedding layer is equal to input_length x embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG16 model\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', \n",
    "                               input_tensor=None, input_shape=None, \n",
    "                               pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Submodel\" of VGG until the fc7 layer\n",
    "vgg_model_fc7 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nasser Benab\\Anaconda2\\lib\\site-packages\\keras\\layers\\recurrent.py:1923: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 22, 300)      3748200     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4096)         134260544   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1024)         5427200     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1024)         0           lstm_1[0][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         1025000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000)         1001000     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1000)         1001000     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 150,658,272\n",
      "Trainable params: 16,397,728\n",
      "Non-trainable params: 134,260,544\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let\"s get a tensor with the output of our vision model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "# Freeze the vision model weights\n",
    "vgg_model_fc7.trainable = False\n",
    "fc7 = vgg_model_fc7(image_input)\n",
    "\n",
    "# Turn the 4096 embedding from the fc7 layer to a 1024 embedding in order to \n",
    "# match the questions embedding\n",
    "encoded_image = Dense(1024, activation=\"tanh\")(fc7)\n",
    "\n",
    "# Langage model\n",
    "question_input = Input(shape=(22,), dtype=\"int32\")\n",
    "embedded_question = Embedding(input_dim=input_dim, output_dim=embedding_dim, \n",
    "                              input_length=input_length)(question_input)\n",
    "encoded_question = LSTM(1024)(embedded_question)\n",
    "\n",
    "# Point-wise multiplication of the outputs from the vision model and the \n",
    "# langage model\n",
    "merge = multiply([encoded_question, encoded_image])\n",
    "\n",
    "# Hidden layers with 0.5 dropouts\n",
    "merge = Dropout(0.5)(merge)\n",
    "hidden1 = Dense(1000, activation=\"tanh\")(merge)\n",
    "hidden1 = Dropout(0.5)(hidden1)\n",
    "hidden2 = Dense(1000, activation=\"tanh\")(hidden1)\n",
    "\n",
    "# Final softmax layer\n",
    "output = Dense(1000, activation=\"softmax\")(hidden2)\n",
    "\n",
    "# This is our final model:\n",
    "vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n",
    "\n",
    "# Summarize layers\n",
    "print(vqa_model.summary())\n",
    "# Plot graph\n",
    "# plot_model(model, to_file=\"vision_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vqa_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split train into a train and val set \n",
    "split_index_1 = 250\n",
    "split_index_2 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [text_vision.train_images[:split_index_1], text_vision.train_sequences[:split_index_1]]\n",
    "x_val = [text_vision.train_images[split_index_1:split_index_2], text_vision.train_sequences[split_index_1:split_index_2]]\n",
    "y_train = text_vision.train_answers_categorical[:split_index_1]\n",
    "y_val = text_vision.train_answers_categorical[split_index_1:split_index_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250L, 224L, 224L, 3L) (250L, 22L) (250L, 1000L)\n",
      "(50L, 224L, 224L, 3L) (50L, 22L) (50L, 1000L)\n"
     ]
    }
   ],
   "source": [
    "print x_train[0].shape, x_train[1].shape, y_train.shape\n",
    "print x_val[0].shape, x_val[1].shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 250 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - ETA: 1:12 - loss: 6.9058 - acc: 0.0000e+0 - 169s 677ms/step - loss: 6.4847 - acc: 0.1480 - val_loss: 10.7840 - val_acc: 0.3200\n"
     ]
    }
   ],
   "source": [
    "history = vqa_model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    }
   ],
   "source": [
    "# List all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8ADlqplY+JugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x186061550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVDiDQJIKZxBIUuEMAkkqnEEgSYX7/yAMAmQI10vjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1866f2198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = vqa_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': u'yes', 'question_id': 1217310},\n",
       " {'answer': u'yes', 'question_id': 1217311},\n",
       " {'answer': u'yes', 'question_id': 4852472},\n",
       " {'answer': u'yes', 'question_id': 4852470},\n",
       " {'answer': u'yes', 'question_id': 4852471},\n",
       " {'answer': u'yes', 'question_id': 2794010},\n",
       " {'answer': u'yes', 'question_id': 2794011},\n",
       " {'answer': u'yes', 'question_id': 2794012},\n",
       " {'answer': u'yes', 'question_id': 4918880},\n",
       " {'answer': u'yes', 'question_id': 4918881},\n",
       " {'answer': u'yes', 'question_id': 4918882},\n",
       " {'answer': u'yes', 'question_id': 5697232},\n",
       " {'answer': u'yes', 'question_id': 5697230},\n",
       " {'answer': u'yes', 'question_id': 5697231},\n",
       " {'answer': u'yes', 'question_id': 5339570},\n",
       " {'answer': u'yes', 'question_id': 5339571},\n",
       " {'answer': u'yes', 'question_id': 5339572},\n",
       " {'answer': u'yes', 'question_id': 4914000},\n",
       " {'answer': u'yes', 'question_id': 4914001},\n",
       " {'answer': u'yes', 'question_id': 4914002},\n",
       " {'answer': u'yes', 'question_id': 4434272},\n",
       " {'answer': u'yes', 'question_id': 4434270},\n",
       " {'answer': u'yes', 'question_id': 4434271},\n",
       " {'answer': u'yes', 'question_id': 5554730},\n",
       " {'answer': u'yes', 'question_id': 5554731},\n",
       " {'answer': u'yes', 'question_id': 5554732},\n",
       " {'answer': u'yes', 'question_id': 3037032},\n",
       " {'answer': u'yes', 'question_id': 3037030},\n",
       " {'answer': u'yes', 'question_id': 3037031},\n",
       " {'answer': u'yes', 'question_id': 2336472},\n",
       " {'answer': u'yes', 'question_id': 2336470},\n",
       " {'answer': u'yes', 'question_id': 2336471},\n",
       " {'answer': u'yes', 'question_id': 1638180},\n",
       " {'answer': u'yes', 'question_id': 1638181},\n",
       " {'answer': u'yes', 'question_id': 1638182},\n",
       " {'answer': u'yes', 'question_id': 208570},\n",
       " {'answer': u'yes', 'question_id': 208571},\n",
       " {'answer': u'yes', 'question_id': 208572},\n",
       " {'answer': u'yes', 'question_id': 5016472},\n",
       " {'answer': u'yes', 'question_id': 5016470},\n",
       " {'answer': u'yes', 'question_id': 5016471},\n",
       " {'answer': u'yes', 'question_id': 1196180},\n",
       " {'answer': u'yes', 'question_id': 1196181},\n",
       " {'answer': u'yes', 'question_id': 1196182},\n",
       " {'answer': u'yes', 'question_id': 5228200},\n",
       " {'answer': u'yes', 'question_id': 5228201},\n",
       " {'answer': u'yes', 'question_id': 5228202},\n",
       " {'answer': u'yes', 'question_id': 3672592},\n",
       " {'answer': u'yes', 'question_id': 3672590},\n",
       " {'answer': u'yes', 'question_id': 3672591}]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = text_vision.predictions_to_dic(predictions, text_vision.train_questions_ids[split_index_1:split_index_2])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'no', u'yes', u'blue', u'brown', u'2', u'yes', u'yes', u'no',\n",
       "       u'donuts', u'no', u'yes', u'yes', u'truck', u'no', u'yes', u'yes',\n",
       "       u'white', u'yes', u'cutting board', u'yes', u'bananas', u'yes',\n",
       "       u'1', u'paper', u'yes', u'no', u'silver', u'2', u'yes', u'yes',\n",
       "       u'scarf', u'clouds', u'8', u'sky', u'metal', u'hit ball', u'yes',\n",
       "       u'red', u'yes', u'sunny', u'plate', u'pizza', u'yes', u'no', u'10',\n",
       "       u'no one', u'black and white', u'no', u'no', u'no'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_ind_val_true = np.argmax(y_val, axis=1)\n",
    "idx_to_answer = np.vectorize(text_vision.idx_to_answer)\n",
    "idx_to_answer(answers_ind_val_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.66666599999999,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
