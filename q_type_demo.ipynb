{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from vqa_api.PythonEvaluationTools.vqaEvaluation.vqaEval import VQAEval\n",
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA\n",
    "from q_type_prior import qTypePrior\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir     = r\"C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa\"\n",
    "# dataDir     = r\"/Users/adib/Documents/Cours 3A/Projet 3A/OR/VQA/vqa_dataset\"\n",
    "versionType = \"\" \n",
    "taskType    = \"OpenEnded\" \n",
    "dataType    = \"mscoco\"\n",
    "dataSubTypes = [\"train2014\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> train2014\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:12.508000\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# First baseline method: random answer\n",
    "qt = qTypePrior(dataDir, versionType, taskType, dataType, dataSubTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top answers per question type\n",
    "qt.get_top_answer_per_qtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.767000\n",
      "creating index...\n",
      "index created!\n",
      "--> Saving the results\n"
     ]
    }
   ],
   "source": [
    "# Predict answers on the validation dataset\n",
    "dataSubType = \"val2014\"\n",
    "qt.predict(dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('qTypePrior results file:', 'C:\\\\Users\\\\Nasser Benab\\\\Documents\\\\git\\\\data\\\\vqa/Results/OpenEnded_mscoco_val2014_qTypePrior_results.json')\n"
     ]
    }
   ],
   "source": [
    "# Name of the baseline method used\n",
    "method_name = qt.__class__.__name__\n",
    "\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType']\n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = \\\n",
    "['%s/Results/%s%s_%s_%s_%s_%s.json'%(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "method_name, fileType) for fileType in fileTypes] \n",
    "print(\"{} results file:\".format(method_name), resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=0.78s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# vqa object for the test \n",
    "vqa = qt.vqa_test\n",
    "# Create vqaRes object\n",
    "vqaRes = vqa.loadRes(resFile, qt.quesFile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing accuracy\n",
      "Finshed Percent: [####################] 99% Done computing accuracy\n",
      "\n",
      "\n",
      "Overall Accuracy is: 35.13\n",
      "\n",
      "Per Question Type Accuracy is the following:\n",
      "are there : 82.13\n",
      "what brand : 11.29\n",
      "what room is : 45.66\n",
      "what color is : 17.81\n",
      "is : 65.84\n",
      "are they : 69.64\n",
      "what number is : 6.32\n",
      "what sport is : 40.71\n",
      "are : 63.23\n",
      "is the : 62.14\n",
      "what is the person : 5.76\n",
      "how many : 38.85\n",
      "does this : 73.04\n",
      "is there a : 88.23\n",
      "is he : 73.37\n",
      "what : 2.04\n",
      "does the : 74.46\n",
      "is the person : 59.19\n",
      "where is the : 4.52\n",
      "what animal is : 17.62\n",
      "how : 5.60\n",
      "what is the woman : 5.53\n",
      "none of the above : 36.90\n",
      "who is : 22.75\n",
      "is the woman : 69.82\n",
      "are the : 61.97\n",
      "how many people are : 37.18\n",
      "what is on the : 4.21\n",
      "has : 70.44\n",
      "was : 57.08\n",
      "what type of : 2.59\n",
      "is this an : 70.60\n",
      "do : 72.10\n",
      "what is the man : 3.82\n",
      "which : 21.56\n",
      "are these : 60.23\n",
      "what are : 2.91\n",
      "what is the : 3.46\n",
      "where are the : 3.87\n",
      "is this a : 62.07\n",
      "can you : 69.36\n",
      "what time : 8.62\n",
      "what are the : 3.25\n",
      "are there any : 66.82\n",
      "what color are the : 23.08\n",
      "why : 13.96\n",
      "what is this : 1.93\n",
      "how many people are in : 34.95\n",
      "do you : 78.64\n",
      "is this : 60.50\n",
      "why is the : 14.67\n",
      "what is the color of the : 23.92\n",
      "what is : 1.79\n",
      "could : 89.91\n",
      "is that a : 63.63\n",
      "what is in the : 4.62\n",
      "what does the : 9.56\n",
      "what kind of : 1.76\n",
      "is it : 63.04\n",
      "is the man : 66.38\n",
      "what is the name : 0.69\n",
      "is there : 80.95\n",
      "what color is the : 21.79\n",
      "what color : 18.53\n",
      "is this person : 61.65\n",
      "\n",
      "\n",
      "Per Answer Type Accuracy is the following:\n",
      "other : 8.86\n",
      "number : 31.93\n",
      "yes/no : 71.33\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate results\n",
    "vqaEval.evaluate() \n",
    "\n",
    "# Print accuracies\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
    "print(\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "    print(\"%s : %.02f\" %(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print(\"\\n\")\n",
    "print(\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "    print(\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "# 35 is per question percentage accuracy\n",
    "# evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   \n",
    "# if len(evals) > 0:\n",
    "#     print('ground truth answers')\n",
    "#     randomEval = random.choice(evals)\n",
    "#     randomAnn = vqa.loadQA(randomEval)\n",
    "#     vqa.showQA(randomAnn)\n",
    "\n",
    "#     print('\\n')\n",
    "#     print('generated answer (accuracy %.02f)'%(vqaEval.evalQA[randomEval]))\n",
    "#     ann = vqaRes.loadQA(randomEval)[0]\n",
    "#     print(\"Answer:   %s\\n\" %(ann['answer']))\n",
    "\n",
    "#     imgId = randomAnn[0]['image_id']\n",
    "#     imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "#     if os.path.isfile(imgDir + imgFilename):\n",
    "#         I = io.imread(imgDir + imgFilename)\n",
    "#         plt.imshow(I)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# # plot accuracy for various question types\n",
    "# plt.bar(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#         list(vqaEval.accuracy['perQuestionType'].values()), align='center')\n",
    "# plt.xticks(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#            list(vqaEval.accuracy['perQuestionType'].keys()), \n",
    "#            rotation='0',fontsize=10)\n",
    "# plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "# plt.xlabel('Question Types', fontsize=10)\n",
    "# plt.ylabel('Accuracy', fontsize=10)\n",
    "# plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "# json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "# json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "# json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "# json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
