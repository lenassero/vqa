{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question_types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-88d0800adc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvqa_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonEvaluationTools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqaEvaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqaEval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQAEval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvqa_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonHelperTools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqaTools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mq_type_prior\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqTypePrior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adib/Documents/Cours 3A/Projet 3A/OR/VQA/vqa/q_type_prior.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mqTypePrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \tdef __init__(self, dataDir, versionType=\"\", taskType=\"OpenEnded\", dataType=\"mscoco\", \n",
      "\u001b[0;32m/Users/adib/Documents/Cours 3A/Projet 3A/OR/VQA/vqa/q_type_prior.py\u001b[0m in \u001b[0;36mqTypePrior\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mq_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mannotation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mq_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_types\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mq_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_types\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question_types' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from vqa_api.PythonEvaluationTools.vqaEvaluation.vqaEval import VQAEval\n",
    "from vqa_api.PythonHelperTools.vqaTools.vqa import VQA\n",
    "from q_type_prior import qTypePrior\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_question_types = os.path.join(dataDir, \"QuestionTypes\")\n",
    "with open(os.path.join(path_question_types, \"mscoco_question_types.txt\")) as f:\n",
    "    question_types = f.read().splitlines()\n",
    "\n",
    "d = {q_type:[annotation[\"answers\"][i][\"answer\"] for i in range(10) for annotation in self.annotations\n",
    "if annotation[\"question_type\"] == q_type] for q_type in question_types}\n",
    "\n",
    "self.top_answers = {q_type:Counter(d[q_type]).most_common(1)[0][0] for q_type in d.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataDir     = r'C:\\Users\\Nasser Benab\\Documents\\git\\data\\vqa'\n",
    "dataDir     = r'/Users/adib/Documents/Cours 3A/Projet 3A/OR/VQA/vqa_dataset'\n",
    "versionType = '' \n",
    "taskType    = 'OpenEnded' \n",
    "dataType    = 'mscoco'\n",
    "dataSubTypes = ['train2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First baseline method: random answer\n",
    "qt = qTypePrior(dataDir, versionType, taskType, dataType, dataSubTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict answers on the validation dataset\n",
    "dataSubType = \"val2014\"\n",
    "qt.predict(dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the baseline method used\n",
    "method_name = qt.__class__.__name__\n",
    "\n",
    "fileTypes   = ['results', 'accuracy', 'evalQA', 'evalQuesType', 'evalAnsType']\n",
    "\n",
    "[resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = \\\n",
    "['%s/Results/%s%s_%s_%s_%s_%s.json'%(dataDir, versionType, taskType, dataType, dataSubType, \\\n",
    "method_name, fileType) for fileType in fileTypes] \n",
    "print(\"{} results file:\".format(method_name), resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vqa object for the test \n",
    "vqa = qt.vqa_test\n",
    "# Create vqaRes object\n",
    "vqaRes = vqa.loadRes(resFile, qt.quesFile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vqaEval object by taking vqa and vqaRes\n",
    "vqaEval = VQAEval(vqa, vqaRes, n=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "vqaEval.evaluate() \n",
    "\n",
    "# Print accuracies\n",
    "print(\"\\n\")\n",
    "print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))\n",
    "print(\"Per Question Type Accuracy is the following:\")\n",
    "for quesType in vqaEval.accuracy['perQuestionType']:\n",
    "    print(\"%s : %.02f\" %(quesType, vqaEval.accuracy['perQuestionType'][quesType]))\n",
    "print(\"\\n\")\n",
    "print(\"Per Answer Type Accuracy is the following:\")\n",
    "for ansType in vqaEval.accuracy['perAnswerType']:\n",
    "    print(\"%s : %.02f\" %(ansType, vqaEval.accuracy['perAnswerType'][ansType]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# demo how to use evalQA to retrieve low score result\n",
    "# 35 is per question percentage accuracy\n",
    "# evals = [quesId for quesId in vqaEval.evalQA if vqaEval.evalQA[quesId]<35]   \n",
    "# if len(evals) > 0:\n",
    "#     print('ground truth answers')\n",
    "#     randomEval = random.choice(evals)\n",
    "#     randomAnn = vqa.loadQA(randomEval)\n",
    "#     vqa.showQA(randomAnn)\n",
    "\n",
    "#     print('\\n')\n",
    "#     print('generated answer (accuracy %.02f)'%(vqaEval.evalQA[randomEval]))\n",
    "#     ann = vqaRes.loadQA(randomEval)[0]\n",
    "#     print(\"Answer:   %s\\n\" %(ann['answer']))\n",
    "\n",
    "#     imgId = randomAnn[0]['image_id']\n",
    "#     imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n",
    "#     if os.path.isfile(imgDir + imgFilename):\n",
    "#         I = io.imread(imgDir + imgFilename)\n",
    "#         plt.imshow(I)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "# # plot accuracy for various question types\n",
    "# plt.bar(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#         list(vqaEval.accuracy['perQuestionType'].values()), align='center')\n",
    "# plt.xticks(list(range(len(vqaEval.accuracy['perQuestionType']))), \n",
    "#            list(vqaEval.accuracy['perQuestionType'].keys()), \n",
    "#            rotation='0',fontsize=10)\n",
    "# plt.title('Per Question Type Accuracy', fontsize=10)\n",
    "# plt.xlabel('Question Types', fontsize=10)\n",
    "# plt.ylabel('Accuracy', fontsize=10)\n",
    "# plt.show()\n",
    "\n",
    "# save evaluation results to ./Results folder\n",
    "# json.dump(vqaEval.accuracy,     open(accuracyFile,     'w'))\n",
    "# json.dump(vqaEval.evalQA,       open(evalQAFile,       'w'))\n",
    "# json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, 'w'))\n",
    "# json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
